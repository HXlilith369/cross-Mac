{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document is for quickly testing a binary classification model. The user can load the best binary classification model and quickly obtain the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for the preprocessed data\n",
    "file_path = '/root/autodl-tmp/test_MOSEI_short_sbert_Hubert_long_features_with_labels.pkl'\n",
    "\n",
    "# Load the file using pickle\n",
    "with open(file_path, 'rb') as file:\n",
    "    features3 = pickle.load(file)\n",
    "\n",
    "# In experiments where data with label 0 is excluded, we only print entries with labels 'Positive' and 'Negative'\n",
    "for i in range(0, len(features3), 3):  # Process in batches of 3 items at a time\n",
    "    for j in range(3):\n",
    "        if i + j >= len(features3):  # Check if there are enough items left\n",
    "            break\n",
    "        item = features3[i + j]\n",
    "        video_id = item.get('video_id', 'N/A')\n",
    "        clip_id = item.get('clip_id', 'N/A')\n",
    "        label_1 = item.get('label_1', 'N/A')\n",
    "        label = item.get('label', 'N/A')\n",
    "        mode = item.get('mode', 'N/A')\n",
    "        text = item.get('text', 'N/A')\n",
    "        text_feature_shape = item['text_feature'].shape if 'text_feature' in item else 'N/A'\n",
    "        audio_feature_shape = item['audio_feature'].shape if 'audio_feature' in item else 'N/A'\n",
    "        vision_feature_shape = item['vision_feature_resnet'].shape if 'vision_feature_resnet' in item else 'N/A'\n",
    "\n",
    "        if label in ['Positive', 'Negative']:\n",
    "            print(f\"Item {i + j}:\")\n",
    "            print(f\"  video_id: {video_id}\")\n",
    "            print(f\"  clip_id: {clip_id}\")\n",
    "            print(f\"  label_1: {label_1}\")\n",
    "            print(f\"  label: {label}\")\n",
    "            print(f\"  mode: {mode}\")\n",
    "            print(f\"  text: {text}\")\n",
    "            print(f\"  text_feature_shape: {text_feature_shape}\")\n",
    "            print(f\"  audio_feature_shape: {audio_feature_shape}\")\n",
    "            print(f\"  vision_feature_shape: {vision_feature_shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out data with 'Positive' or 'Negative' labels and 'test' mode\n",
    "test_data = [item for item in features3 if item['mode'] == 'test' and item['label'] in ['Positive', 'Negative']]\n",
    "\n",
    "# Get the labels from the test data\n",
    "test_y = np.array([item['label'] for item in test_data])\n",
    "\n",
    "# Get text features from the test data\n",
    "test_X_text = [item['text_feature'] for item in test_data]\n",
    "\n",
    "# Get audio features from the test data\n",
    "test_X_audio = [item['audio_feature'] for item in test_data]\n",
    "\n",
    "# Get vision features from the test data\n",
    "test_X_vision = [item['vision_feature_resnet'] for item in test_data]\n",
    "\n",
    "\n",
    "# Initialize label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Transform labels in test_y to numeric labels\n",
    "test_Y = label_encoder.fit_transform(test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Put all audio-feature data into a dictionary\n",
    "text_features = {\n",
    "    'test_audio': test_X_audio\n",
    "}\n",
    "\n",
    "# Iterate over the dictionary, convert to tensor and remove dimensions, then print the shape\n",
    "for key, features in text_features.items():\n",
    "    for i, feature in enumerate(features):\n",
    "        tensor_feature = torch.tensor(feature)  # Convert feature to tensor\n",
    "        tensor_feature = tensor_feature.squeeze(0)  # Remove the dimension of size 1 (assuming it's the first dimension)\n",
    "        text_features[key][i] = tensor_feature  # Update the value in the dictionary\n",
    "        print(f\"Feature in {key} at index {i} has shape: {tensor_feature.shape}\")\n",
    "\n",
    "# Put all text_feature data into a dictionary\n",
    "text_features = {\n",
    "    'test_text': test_X_text\n",
    "}\n",
    "# Iterate over the dictionary, convert to tensor, remove dimensions, and print the shape\n",
    "for key, features in text_features.items():\n",
    "    for i, feature in enumerate(features):\n",
    "        tensor_feature = torch.tensor(feature)  # Convert feature to tensor\n",
    "        tensor_feature = tensor_feature.squeeze(0)  # Remove the dimension of size 1 (assuming it's the first dimension)\n",
    "        text_features[key][i] = tensor_feature  # Update the value in the dictionary\n",
    "        print(f\"Feature in {key} at index {i} has shape: {tensor_feature.shape}\")\n",
    "\n",
    "# Put all vision_feature data into a dictionary\n",
    "vision_features = {\n",
    "    'test_vision': test_X_vision\n",
    "}\n",
    "\n",
    "# Iterate over the dictionary, convert to tensor, and print the shape\n",
    "for key, features in vision_features.items():\n",
    "    for i, feature in enumerate(features):\n",
    "        tensor_feature = torch.tensor(feature)  # Convert feature to tensor\n",
    "        vision_features[key][i] = tensor_feature  # Update the value in the dictionary\n",
    "        print(f\"Feature in {key} at index {i} has shape: {tensor_feature.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeModal_Dataset(Dataset):\n",
    "    def __init__(self, text_features, audio_features, vision_features, labels):\n",
    "        # Initialize the dataset with text, audio, vision features, and labels\n",
    "        self.text_features = text_features\n",
    "        self.audio_features = audio_features\n",
    "        self.vision_features = vision_features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the length of the dataset (number of samples)\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the features (text, audio, vision) and label for the given index\n",
    "        text_features = self.text_features[idx]\n",
    "        audio_features = self.audio_features[idx]\n",
    "        vision_features = self.vision_features[idx]\n",
    "        labels = self.labels[idx]\n",
    "        return text_features, audio_features, vision_features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Separate audio features, text features, and labels from the batch\n",
    "    text_features, audio_features, vision_features, labels = zip(*batch)\n",
    "    \n",
    "    # Pad the audio features and text features to the length of the longest sequence in the batch\n",
    "    text_features_padded = pad_sequence(text_features, batch_first=True)    \n",
    "    audio_features_padded = pad_sequence(audio_features, batch_first=True)\n",
    "    vision_features_padded = pad_sequence(vision_features, batch_first=True)    \n",
    "    \n",
    "    # Convert labels to a tensor\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return text_features_padded, audio_features_padded, vision_features_padded, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test labels to a tensor\n",
    "test_Y_tensor = torch.tensor(test_Y)\n",
    "\n",
    "# Create a dataset for the test data using ThreeModal_Dataset class\n",
    "test_dataset = ThreeModal_Dataset(test_X_text, test_X_audio, test_X_vision, test_Y_tensor)\n",
    "\n",
    "# Create a DataLoader for the test dataset with a batch size of 8, without shuffling, \n",
    "# and using 2 worker processes for loading the data, with a custom collate function\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleCrossAttentionClassifier(nn.Module):\n",
    "    def __init__(self, text_dim, audio_dim, vision_dim, hidden_dim, num_heads, dropout_rate=0.1):\n",
    "        super(SimpleCrossAttentionClassifier, self).__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Linear projection of text and audio/vision to map to a unified hidden dimension\n",
    "        self.text_to_hidden = nn.Linear(text_dim, hidden_dim)\n",
    "        self.audio_to_hidden = nn.Linear(audio_dim, hidden_dim)\n",
    "        self.vision_to_hidden = nn.Linear(vision_dim, hidden_dim)\n",
    "\n",
    "        # Cross-Attention between Text-Audio and Vision-Text\n",
    "        self.cross_attention_text_audio = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, dropout=dropout_rate)\n",
    "        self.cross_attention_vision_text = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, dropout=dropout_rate)\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, 1)  # Concatenation of two Cross-Attention outputs\n",
    "\n",
    "    def forward(self, text_features, audio_features, vision_features):\n",
    "        # Project to hidden dimension\n",
    "        text_hidden = self.text_to_hidden(text_features)  # (batch_size, seq_len, hidden_dim)\n",
    "        audio_hidden = self.audio_to_hidden(audio_features)  # (batch_size, seq_len, hidden_dim)\n",
    "        vision_hidden = self.vision_to_hidden(vision_features)  # (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        # Reshape to (seq_len, batch_size, hidden_dim) to fit MultiheadAttention\n",
    "        text_hidden = text_hidden.permute(1, 0, 2)\n",
    "        audio_hidden = audio_hidden.permute(1, 0, 2)\n",
    "        vision_hidden = vision_hidden.permute(1, 0, 2)\n",
    "\n",
    "        # Text-Audio Cross-Attention\n",
    "        text_audio_output, _ = self.cross_attention_text_audio(query=text_hidden, key=audio_hidden, value=audio_hidden)\n",
    "        text_audio_output = self.dropout(text_audio_output)  # Apply Dropout\n",
    "        text_audio_output = text_audio_output.permute(1, 0, 2)  # Reshape back to (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        # Vision-Text Cross-Attention\n",
    "        vision_text_output, _ = self.cross_attention_vision_text(query=vision_hidden, key=text_hidden, value=text_hidden)\n",
    "        vision_text_output = self.dropout(vision_text_output)  # Apply Dropout\n",
    "        vision_text_output = vision_text_output.permute(1, 0, 2)  # Reshape back to (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        # Contrastive learning - using InfoNCE loss\n",
    "        batch_size = text_audio_output.size(0)\n",
    "\n",
    "        # Normalize the output with L2 normalization to compute cosine similarity\n",
    "        text_audio_norm = F.normalize(text_audio_output.mean(dim=1), p=2, dim=-1)\n",
    "        vision_text_norm = F.normalize(vision_text_output.mean(dim=1), p=2, dim=-1)\n",
    "\n",
    "        # Compute the similarity matrix\n",
    "        similarity_matrix = torch.matmul(text_audio_norm, vision_text_norm.T)\n",
    "\n",
    "        # Compute similarity for positive samples (diagonal)\n",
    "        positive_samples = torch.diag(similarity_matrix)\n",
    "\n",
    "        # Compute InfoNCE loss\n",
    "        temperature = 0.07  # Temperature parameter, adjustable\n",
    "        similarity_matrix /= temperature\n",
    "        positive_samples /= temperature\n",
    "\n",
    "        # InfoNCE loss computation\n",
    "        contrastive_loss = -torch.log(\n",
    "            torch.exp(positive_samples) / torch.exp(similarity_matrix).sum(dim=1)\n",
    "        ).mean()\n",
    "\n",
    "        # Pooling operation: take the average across the sequence dimension\n",
    "        text_audio_pooled = text_audio_output.mean(dim=1)  # (batch_size, hidden_dim)\n",
    "        vision_text_pooled = vision_text_output.mean(dim=1)  # (batch_size, hidden_dim)\n",
    "\n",
    "        # Concatenate Text-Audio and Vision-Text Cross-Attention outputs\n",
    "        combined_output = torch.cat((text_audio_pooled, vision_text_pooled), dim=1)  # (batch_size, hidden_dim * 2)\n",
    "\n",
    "        # Classification\n",
    "        output = self.classifier(combined_output)\n",
    "        output = torch.sigmoid(output)  # Binary classification\n",
    "\n",
    "        return output, contrastive_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleCrossAttentionClassifier(\n",
    "    text_dim=768,\n",
    "    audio_dim=1024,\n",
    "    vision_dim=2048,\n",
    "    hidden_dim=256,\n",
    "    num_heads=8,\n",
    "    dropout_rate=0.01\n",
    ").to(device)\n",
    "# Load the best model weights\n",
    "best_model_path = '/root/autodl-tmp/best_model_three_modal.pth' #You can enter the local path of the best model.\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Initialize lists to collect true labels and predicted labels\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "with torch.no_grad():\n",
    "    for audio_features, text_features, vision_features, labels in test_loader:  # Assuming you have vision_features\n",
    "        # Move audio, text, vision features, and labels to GPU\n",
    "        audio_features = audio_features.to(device)\n",
    "        text_features = text_features.to(device)\n",
    "        vision_features = vision_features.to(device)  # Includes vision features\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, _ = model(audio_features, text_features, vision_features)  # If the model has contrastive learning part, we ignore the loss\n",
    "\n",
    "        # Get predictions and convert to numpy array\n",
    "        predicted = torch.sigmoid(outputs.squeeze()).round().cpu().numpy()  # Binary classification, predictions are 0 or 1\n",
    "        predicted_labels.extend(predicted)\n",
    "        \n",
    "        # Convert true labels to numpy array\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "true_labels = np.array(true_labels)\n",
    "predicted_labels = np.array(predicted_labels)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels, average='binary')\n",
    "recall = recall_score(true_labels, predicted_labels, average='binary')\n",
    "f1 = f1_score(true_labels, predicted_labels, average='binary')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Prevent division by zero and calculate UA (Unweighted Average) and WA (Weighted Average)\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    per_class_accuracy = np.diag(cm) / np.sum(cm, axis=1)\n",
    "    per_class_accuracy = np.nan_to_num(per_class_accuracy)  # Convert NaN to 0 to avoid computation errors\n",
    "    ua = np.mean(per_class_accuracy)  # Unweighted Average (UA)\n",
    "\n",
    "wa = accuracy  # Weighted Average (WA), for binary classification, weighted average equals overall accuracy\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Unweighted Average (UA): {ua:.4f}\")\n",
    "print(f\"Weighted Average (WA): {wa:.4f}\")\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"\\nDetailed classification report:\")\n",
    "print(classification_report(true_labels, predicted_labels, target_names=['Class 0', 'Class 1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleCrossAttentionClassifier(\n",
    "    text_dim=768,\n",
    "    audio_dim=1024,\n",
    "    vision_dim=2048,\n",
    "    hidden_dim=256,\n",
    "    num_heads=8,\n",
    "    dropout_rate=0.01\n",
    ").to(device)\n",
    "# Load the best model weights\n",
    "best_model_path = '/root/autodl-tmp/best_model_three_modal.pth'\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Initialize lists to collect true labels and predicted labels\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "with torch.no_grad():\n",
    "    for audio_features, text_features, vision_features, labels in test_loader:  # Assuming you have vision_features\n",
    "        # Move audio, text, vision features, and labels to GPU\n",
    "        audio_features = audio_features.to(device)\n",
    "        text_features = text_features.to(device)\n",
    "        vision_features = vision_features.to(device)  # Includes vision features\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, _ = model(audio_features, text_features, vision_features)  # If the model has contrastive learning part, we ignore the loss\n",
    "\n",
    "        # Get predictions and convert to numpy array\n",
    "        predicted = torch.sigmoid(outputs.squeeze()).round().cpu().numpy()  # Binary classification, predictions are 0 or 1\n",
    "        predicted_labels.extend(predicted)\n",
    "        \n",
    "        # Convert true labels to numpy array\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "true_labels = np.array(true_labels)\n",
    "predicted_labels = np.array(predicted_labels)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels, average='binary')\n",
    "recall = recall_score(true_labels, predicted_labels, average='binary')\n",
    "f1 = f1_score(true_labels, predicted_labels, average='binary')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Prevent division by zero and calculate UA (Unweighted Average) and WA (Weighted Average)\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    per_class_accuracy = np.diag(cm) / np.sum(cm, axis=1)\n",
    "    per_class_accuracy = np.nan_to_num(per_class_accuracy)  # Convert NaN to 0 to avoid computation errors\n",
    "    ua = np.mean(per_class_accuracy)  # Unweighted Average (UA)\n",
    "\n",
    "wa = accuracy  # Weighted Average (WA), for binary classification, weighted average equals overall accuracy\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Unweighted Average (UA): {ua:.4f}\")\n",
    "print(f\"Weighted Average (WA): {wa:.4f}\")\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"\\nDetailed classification report:\")\n",
    "print(classification_report(true_labels, predicted_labels, target_names=['Class 0', 'Class 1']))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
